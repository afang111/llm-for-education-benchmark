import json
import os

print(os.environ.get("ANTHROPIC_API_KEY"))

from langchain_core.prompts import ChatPromptTemplate
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI

# ç¡®è®¤ API Key
api_key = os.environ.get("ANTHROPIC_API_KEY")
if not api_key:
    raise ValueError("âŒ æ²¡æœ‰æ£€æµ‹åˆ° ANTHROPIC_API_KEYï¼Œè¯·å…ˆè¿è¡Œ export ANTHROPIC_API_KEY=sk-xxxx")

# === 1. åŠ è½½æ•°æ®é›† ===
with open("fractions_benchmark.json", "r") as f:
    dataset = json.load(f)

# === 2. åˆå§‹åŒ– Claude æ¨¡å‹ ===
llm = ChatAnthropic(
    model="claude-3-sonnet-20240229",   # ä½ ç”¨çš„æ¨¡å‹åå­—
    temperature=0,
    max_tokens=512
)

# å®šä¹‰ä¸€ä¸ªç»Ÿä¸€çš„ prompt
prompt = ChatPromptTemplate.from_template(
    """ä½ æ˜¯ä¸€ä¸ªåˆ†æ•°è®¡ç®—è€å¸ˆã€‚è¯·è§£ç­”ä¸‹é¢çš„é¢˜ç›®ï¼Œå¹¶è¾“å‡º JSON æ ¼å¼ç»“æœã€‚
é¢˜ç›®: {question}

ä¸¥æ ¼è¾“å‡ºè¿™ä¸ª JSONï¼š
{{
  "answer": "...",
  "skills": ["Î±1","Î±4"],
  "explanation": "é€æ­¥å†™å‡ºè§£é¢˜è¿‡ç¨‹ï¼Œå¹¶æŒ‡å‡ºç”¨äº†å“ªäº›æŠ€èƒ½"
}}"""
)

# === 3. è°ƒç”¨æ™ºèƒ½ä½“ ===
def solve(question: str):
    chain = prompt | llm
    resp = chain.invoke({"question": question})
    content = resp.content.strip()

    try:
        result = json.loads(content)
    except Exception:
        # å¦‚æœ Claude æ²¡æœ‰è¾“å‡º JSONï¼Œå°±åŒ…è£…ä¸€ä¸‹
        result = {"answer": "", "skills": [], "explanation": content}

    return {
        "answer": result.get("answer", ""),
        "skills_used": result.get("skills", []),
        "explanation": result.get("explanation", "")
    }

# === 4. è¯„æµ‹å‡½æ•° ===
def evaluate(example, result):
    expected_skills = set(example.get("skills", []))
    alt_skills = set(example.get("alt_skills", []))

    answer_ok = (result["answer"] == example.get("answer", None))
    skills_pred = set(result["skills_used"])
    skill_ok = (skills_pred == expected_skills) or (skills_pred == alt_skills)
    explanation_ok = all(s in result.get("explanation", "") for s in expected_skills)

    return {
        "id": example["id"],
        "question": example["question"],
        "expected_answer": example.get("answer", None),
        "model_answer": result["answer"],
        "expected_skills": list(expected_skills),
        "model_skills": result["skills_used"],
        "correct": answer_ok,
        "skill_ok": skill_ok,
        "process_ok": explanation_ok,
        "explanation": result.get("explanation", "")
    }

# === 5. è·‘ benchmark ===
results = []
for ex in dataset:
    res = solve(ex["question"])
    results.append(evaluate(ex, res))

accuracy = sum(r["correct"] for r in results) / len(results)
skill_match = sum(r["skill_ok"] for r in results) / len(results)
process_match = sum(r["process_ok"] for r in results) / len(results)

print("\nğŸ“‘ Detailed Results:")
for r in results:
    errors = []
    if not r["correct"]:
        errors.append("Answer Wrong")
    if not r["skill_ok"]:
        errors.append("Skill Wrong")
    if not r["process_ok"]:
        errors.append("Process Wrong")

    if not errors:
        status = "âœ… PASS"
    else:
        status = "âŒ FAIL (" + ", ".join(errors) + ")"

    print(f"\n{status} | {r['id']} | {r['question']}")
    print(f"  Expected Answer: {r['expected_answer']}")
    print(f"  Model Answer:   {r['model_answer']}")
    print(f"  Expected Skills: {r['expected_skills']}")
    print(f"  Model Skills:    {r['model_skills']}")
    print(f"  Explanation: {r['explanation']}")
